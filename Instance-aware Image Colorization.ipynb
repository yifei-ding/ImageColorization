{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Instance-aware Image Colorization.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1wvsZSscOlYsova84Mmg7YNwQwLC_EMaX","authorship_tag":"ABX9TyMlw9cwuQAo0sRvDiAxsl6g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"POIgWlxSsW29"},"source":["# Environemnt Setting"]},{"cell_type":"code","metadata":{"id":"v0IZeX7UsT5K"},"source":["!pip install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html\n","!pip install cython pyyaml==5.1\n","!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n","!pip install dominate==2.4.0\n","!pip install detectron2==0.1.2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0-x5QPscNiHF"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"c3p4qOJduONg"},"source":["# Switch to local drive and clone code"]},{"cell_type":"code","metadata":{"id":"uwUL5nVssjn0"},"source":["import os\n","os.chdir(\"/content/drive/MyDrive/02456 Deep Learning - Image Colorization\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SsrP7-8btdj9"},"source":["#!git clone https://github.com/ericsujw/InstColorization.git"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NPDS3jBLuXzn"},"source":["# Download pre-trained weights"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RuZ0Pb8OtkFv","executionInfo":{"status":"ok","timestamp":1637440878673,"user_tz":-60,"elapsed":3,"user":{"displayName":"Ding Yifei","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14157177939779618244"}},"outputId":"868384db-6410-4ed1-ae78-eea1be2c98bc"},"source":["cd InstColorization/\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/02456 Deep Learning - Image Colorization/InstColorization\n"]}]},{"cell_type":"code","metadata":{"id":"VhdsVAAJtnYu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636729796046,"user_tz":-60,"elapsed":44184,"user":{"displayName":"Ding Yifei","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14157177939779618244"}},"outputId":"edc7399c-35e7-41aa-8e94-6d24449d078f"},"source":["!sh scripts/download_model.sh"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","Finish download.\n","Archive:  checkpoints.zip\n","   creating: checkpoints/coco_finetuned_mask_256_ffs/\n","  inflating: checkpoints/coco_finetuned_mask_256_ffs/latest_net_GComp.pth  \n","  inflating: checkpoints/coco_finetuned_mask_256_ffs/latest_net_G.pth  \n","  inflating: checkpoints/coco_finetuned_mask_256_ffs/latest_net_GF.pth  \n","   creating: checkpoints/coco_finetuned_mask_256/\n","  inflating: checkpoints/coco_finetuned_mask_256/latest_net_GComp.pth  \n","  inflating: checkpoints/coco_finetuned_mask_256/latest_net_G.pth  \n","  inflating: checkpoints/coco_finetuned_mask_256/latest_net_GF.pth  \n","   creating: checkpoints/siggraph_retrained/\n","  inflating: checkpoints/siggraph_retrained/latest_net_G.pth  \n"]}]},{"cell_type":"markdown","metadata":{"id":"5RJjQa7rudqV"},"source":["# Prepare dataset"]},{"cell_type":"code","metadata":{"id":"e3sqXkBItwbr"},"source":["#!sh scripts/prepare_cocostuff.sh "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uAHRrIFuNel9"},"source":["import os\n","import glob\n","import time\n","import numpy as np\n","from PIL import Image\n","from pathlib import Path\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","from skimage.color import rgb2lab, lab2rgb\n","\n","import torch\n","from torch import nn, optim\n","from torchvision import transforms\n","from torchvision.utils import make_grid\n","from torch.utils.data import Dataset, DataLoader\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","use_colab = None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"yQDSqUlMiE8Y","executionInfo":{"status":"ok","timestamp":1636408465884,"user_tz":-60,"elapsed":170602,"user":{"displayName":"Ding Yifei","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14157177939779618244"}},"outputId":"8522bf96-8d7b-433b-e42f-b8534202f367"},"source":["!pip install fastai --upgrade\n","\n","from fastai.data.external import untar_data, URLs\n","coco_path = untar_data(URLs.COCO_SAMPLE)\n","coco_path = str(coco_path) + \"/train_sample\"\n","use_colab = True"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='3245883392' class='' max='3245877008' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [3245883392/3245877008 01:58<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RNaLTmIejJHM","executionInfo":{"status":"ok","timestamp":1636408545512,"user_tz":-60,"elapsed":241,"user":{"displayName":"Ding Yifei","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14157177939779618244"}},"outputId":"b34c9fc6-b892-4a2b-cc82-67a903d1b325"},"source":["if use_colab == True:\n","    path = coco_path\n","else:\n","    path = \"Your path to the dataset\"\n","    \n","paths = glob.glob(path + \"/*.jpg\") # Grabbing all the image file names\n","np.random.seed(123)\n","paths_subset = np.random.choice(paths, 10_000, replace=False) # choosing 1000 images randomly\n","rand_idxs = np.random.permutation(10_000)\n","train_idxs = rand_idxs[:8000] # choosing the first 8000 as training set\n","val_idxs = rand_idxs[8000:] # choosing last 2000 as validation set\n","train_paths = paths_subset[train_idxs]\n","val_paths = paths_subset[val_idxs]\n","print(len(train_paths), len(val_paths))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["8000 2000\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uIARNw_8jtj1","executionInfo":{"status":"ok","timestamp":1636409213487,"user_tz":-60,"elapsed":216,"user":{"displayName":"Ding Yifei","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14157177939779618244"}},"outputId":"971c3607-803d-4c0c-ffea-a1d0f7b66fbc"},"source":["print(train_paths[0:5])\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['/root/.fastai/data/coco_sample/train_sample/000000388312.jpg'\n"," '/root/.fastai/data/coco_sample/train_sample/000000533548.jpg'\n"," '/root/.fastai/data/coco_sample/train_sample/000000500646.jpg'\n"," '/root/.fastai/data/coco_sample/train_sample/000000151170.jpg'\n"," '/root/.fastai/data/coco_sample/train_sample/000000263434.jpg']\n"]}]},{"cell_type":"code","metadata":{"id":"8NdlopLsmpbm"},"source":["!cp '/root/.fastai/data/coco_sample/train_sample/000000388312.jpg' \"/content/drive/MyDrive/02456 Deep Learning - Image Colorization/InstColorization/train_data/cocostuff8000\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u55ihVv5nsFN"},"source":["for file in train_paths:\n","  !cp $file \"/content/drive/MyDrive/02456 Deep Learning - Image Colorization/InstColorization/train_data/cocostuff8000\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lI0NKTSyNShg"},"source":["# Object Detection"]},{"cell_type":"code","metadata":{"id":"brAaNkLYwdB8"},"source":["!sh scripts/prepare_train_box.sh #bounding box"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HR8FqlDYKAvL"},"source":["# Define the model"]},{"cell_type":"markdown","metadata":{"id":"BqFpMqRCK1-h"},"source":["1. _init_.py"]},{"cell_type":"code","metadata":{"id":"Ayx4JRduKzkq"},"source":["import importlib\n","from models.base_model import BaseModel\n","\n","\n","def find_model_using_name(model_name):\n","    # Given the option --model [modelname],\n","    # the file \"models/modelname_model.py\"\n","    # will be imported.\n","    model_filename = \"models.\" + model_name + \"_model\"\n","    modellib = importlib.import_module(model_filename)\n","\n","    # In the file, the class called ModelNameModel() will\n","    # be instantiated. It has to be a subclass of BaseModel,\n","    # and it is case-insensitive.\n","    model = None\n","    target_model_name = model_name.replace('_', '') + 'model'\n","    for name, cls in modellib.__dict__.items():\n","        if name.lower() == target_model_name.lower() \\\n","           and issubclass(cls, BaseModel):\n","            model = cls\n","\n","    if model is None:\n","        print(\"In %s.py, there should be a subclass of BaseModel with class name that matches %s in lowercase.\" % (model_filename, target_model_name))\n","        exit(0)\n","\n","    return model\n","\n","\n","def get_option_setter(model_name):\n","    model_class = find_model_using_name(model_name)\n","    return model_class.modify_commandline_options\n","\n","\n","def create_model(opt):\n","    model = find_model_using_name(opt.model)\n","    instance = model()\n","    instance.initialize(opt)\n","    print(\"model [%s] was created\" % (instance.name()))\n","    return instance"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m7kaZ-b6K-zi"},"source":["2. BaseModel"]},{"cell_type":"code","metadata":{"id":"BsN_uDIFLArp"},"source":["import os\n","import torch\n","from collections import OrderedDict\n","from . import networks\n","\n","\n","class BaseModel():\n","\n","    # modify parser to add command line options,\n","    # and also change the default values if needed\n","    @staticmethod\n","    def modify_commandline_options(parser, is_train):\n","        return parser\n","\n","    def name(self):\n","        return 'BaseModel'\n","\n","    def initialize(self, opt):\n","        self.opt = opt\n","        self.gpu_ids = opt.gpu_ids\n","        self.isTrain = opt.isTrain\n","        self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')\n","        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n","        if opt.resize_or_crop != 'scale_width':\n","            torch.backends.cudnn.benchmark = True\n","        self.loss_names = []\n","        self.model_names = []\n","        self.visual_names = []\n","        self.image_paths = []\n","\n","    def set_input(self, input):\n","        self.input = input\n","\n","    def forward(self):\n","        pass\n","\n","    # load and print networks; create schedulers\n","    def setup(self, opt, parser=None):\n","        if self.isTrain:\n","            self.schedulers = [networks.get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n","\n","        if not self.isTrain or opt.load_model:\n","            self.load_networks(opt.which_epoch)\n","\n","    # make models eval mode during test time\n","    def eval(self):\n","        for name in self.model_names:\n","            if isinstance(name, str):\n","                net = getattr(self, 'net' + name)\n","                net.eval()\n","\n","    # used in test time, wrapping `forward` in no_grad() so we don't save\n","    # intermediate steps for backprop\n","    def test(self, compute_losses=False):\n","        with torch.no_grad():\n","            self.forward()\n","            if(compute_losses):\n","                self.compute_losses_G()\n","\n","    # get image paths\n","    def get_image_paths(self):\n","        return self.image_paths\n","\n","    def optimize_parameters(self):\n","        pass\n","\n","    # update learning rate (called once every epoch)\n","    def update_learning_rate(self):\n","        for scheduler in self.schedulers:\n","            scheduler.step()\n","        lr = self.optimizers[0].param_groups[0]['lr']\n","        # print('learning rate = %.7f' % lr)\n","\n","    # return visualization images. train.py will display these images, and save the images to a html\n","    def get_current_visuals(self):\n","        visual_ret = OrderedDict()\n","        for name in self.visual_names:\n","            if isinstance(name, str):\n","                visual_ret[name] = getattr(self, name)\n","        return visual_ret\n","\n","    # return traning losses/errors. train.py will print out these errors as debugging information\n","    def get_current_losses(self):\n","        errors_ret = OrderedDict()\n","        for name in self.loss_names:\n","            if isinstance(name, str):\n","                # float(...) works for both scalar tensor and float number\n","                errors_ret[name] = float(getattr(self, 'loss_' + name))\n","        return errors_ret\n","\n","    # save models to the disk\n","    def save_networks(self, which_epoch):\n","        for name in self.model_names:\n","            if isinstance(name, str):\n","                save_filename = '%s_net_%s.pth' % (which_epoch, name)\n","                save_path = os.path.join(self.save_dir, save_filename)\n","                net = getattr(self, 'net' + name)\n","\n","                if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n","                    torch.save(net.module.cpu().state_dict(), save_path)\n","                    net.cuda(self.gpu_ids[0])\n","                else:\n","                    torch.save(net.cpu().state_dict(), save_path)\n","\n","    def __patch_instance_norm_state_dict(self, state_dict, module, keys, i=0):\n","        key = keys[i]\n","        if i + 1 == len(keys):  # at the end, pointing to a parameter/buffer\n","            if module.__class__.__name__.startswith('InstanceNorm') and \\\n","                    (key == 'running_mean' or key == 'running_var'):\n","                if getattr(module, key) is None:\n","                    state_dict.pop('.'.join(keys))\n","            if module.__class__.__name__.startswith('InstanceNorm') and \\\n","               (key == 'num_batches_tracked'):\n","                state_dict.pop('.'.join(keys))\n","        else:\n","            self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)\n","\n","    # load models from the disk\n","    def load_networks(self, which_epoch):\n","        for name in self.model_names:\n","            if isinstance(name, str):\n","                load_filename = '%s_net_%s.pth' % (which_epoch, name)\n","                load_path = os.path.join(self.save_dir, load_filename)\n","                if os.path.isfile(load_path) is False:\n","                    continue\n","                net = getattr(self, 'net' + name)\n","                if isinstance(net, torch.nn.DataParallel):\n","                    net = net.module\n","                print('loading the model from %s' % load_path)\n","                # if you are using PyTorch newer than 0.4 (e.g., built from\n","                # GitHub source), you can remove str() on self.device\n","                state_dict = torch.load(load_path, map_location=str(self.device))\n","                if hasattr(state_dict, '_metadata'):\n","                    del state_dict._metadata\n","\n","                # patch InstanceNorm checkpoints prior to 0.4\n","                # for key in list(state_dict.keys()):  # need to copy keys here because we mutate in loop\n","                #     self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))\n","                net.load_state_dict(state_dict, strict=False)\n","\n","    # print network information\n","    def print_networks(self, verbose):\n","        print('---------- Networks initialized -------------')\n","        for name in self.model_names:\n","            if isinstance(name, str):\n","                net = getattr(self, 'net' + name)\n","                num_params = 0\n","                for param in net.parameters():\n","                    num_params += param.numel()\n","                if verbose:\n","                    print(net)\n","                print('[Network %s] Total number of parameters : %.3f M' % (name, num_params / 1e6))\n","        print('-----------------------------------------------')\n","\n","    # set requies_grad=Fasle to avoid computation\n","    def set_requires_grad(self, nets, requires_grad=False):\n","        if not isinstance(nets, list):\n","            nets = [nets]\n","        for net in nets:\n","            if net is not None:\n","                for param in net.parameters():\n","                    param.requires_grad = requires_grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JMn2Q0PCLF_Y"},"source":["3. Train model"]},{"cell_type":"code","metadata":{"id":"2sdLlRMWLHvG"},"source":["import os\n","\n","import torch\n","from collections import OrderedDict\n","from util.image_pool import ImagePool\n","from util import util\n","from .base_model import BaseModel\n","from . import networks\n","import numpy as np\n","from skimage import io\n","from skimage import img_as_ubyte\n","\n","import matplotlib.pyplot as plt\n","import math\n","from matplotlib import colors\n","\n","\n","class TrainModel(BaseModel):\n","    def name(self):\n","        return 'TrainModel'\n","\n","    @staticmethod\n","    def modify_commandline_options(parser, is_train=True):\n","        return parser\n","\n","    def initialize(self, opt):\n","        BaseModel.initialize(self, opt)\n","        self.loss_names = ['G', 'L1']\n","        # load/define networks\n","        num_in = opt.input_nc + opt.output_nc + 1\n","        self.optimizers = []\n","        if opt.stage == 'full' or opt.stage == 'instance':\n","            self.model_names = ['G']\n","            self.netG = networks.define_G(num_in, opt.output_nc, opt.ngf,\n","                                        'siggraph', opt.norm, not opt.no_dropout, opt.init_type, self.gpu_ids,\n","                                        use_tanh=True, classification=opt.classification)\n","            self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n","                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n","            self.optimizers.append(self.optimizer_G)\n","        elif opt.stage == 'fusion':\n","            self.model_names = ['G', 'GF', 'GComp']\n","            self.netG = networks.define_G(num_in, opt.output_nc, opt.ngf,\n","                                        'instance', opt.norm, not opt.no_dropout, opt.init_type, self.gpu_ids,\n","                                        use_tanh=True, classification=False)\n","            self.netG.eval()\n","            \n","            self.netGF = networks.define_G(num_in, opt.output_nc, opt.ngf,\n","                                        'fusion', opt.norm, not opt.no_dropout, opt.init_type, self.gpu_ids,\n","                                        use_tanh=True, classification=False)\n","            self.netGF.eval()\n","\n","            self.netGComp = networks.define_G(num_in, opt.output_nc, opt.ngf,\n","                                        'siggraph', opt.norm, not opt.no_dropout, opt.init_type, self.gpu_ids,\n","                                        use_tanh=True, classification=opt.classification)\n","            self.netGComp.eval()\n","            self.optimizer_G = torch.optim.Adam(list(self.netGF.module.weight_layer.parameters()) +\n","                                                list(self.netGF.module.weight_layer2.parameters()) +\n","                                                list(self.netGF.module.weight_layer3.parameters()) +\n","                                                list(self.netGF.module.weight_layer4.parameters()) +\n","                                                list(self.netGF.module.weight_layer5.parameters()) +\n","                                                list(self.netGF.module.weight_layer6.parameters()) +\n","                                                list(self.netGF.module.weight_layer7.parameters()) +\n","                                                list(self.netGF.module.weight_layer8_1.parameters()) +\n","                                                list(self.netGF.module.weight_layer8_2.parameters()) +\n","                                                list(self.netGF.module.weight_layer9_1.parameters()) +\n","                                                list(self.netGF.module.weight_layer9_2.parameters()) +\n","                                                list(self.netGF.module.weight_layer10_1.parameters()) +\n","                                                list(self.netGF.module.weight_layer10_2.parameters()) +\n","                                                list(self.netGF.module.model10.parameters()) +\n","                                                list(self.netGF.module.model_out.parameters()),\n","                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n","            self.optimizers.append(self.optimizer_G)\n","        else:\n","            print('Error Stage!')\n","            exit()\n","        self.criterionL1 = networks.HuberLoss(delta=1. / opt.ab_norm)\n","        # self.criterionL1 = networks.L1Loss()\n","\n","        # initialize average loss values\n","        self.avg_losses = OrderedDict()\n","        self.avg_loss_alpha = opt.avg_loss_alpha\n","        self.error_cnt = 0\n","        for loss_name in self.loss_names:\n","            self.avg_losses[loss_name] = 0\n","        \n","    def set_input(self, input):\n","        AtoB = self.opt.which_direction == 'AtoB'\n","        self.real_A = input['A' if AtoB else 'B'].to(self.device)\n","        self.real_B = input['B' if AtoB else 'A'].to(self.device)\n","        self.hint_B = input['hint_B'].to(self.device)\n","        \n","        self.mask_B = input['mask_B'].to(self.device)\n","        self.mask_B_nc = self.mask_B + self.opt.mask_cent\n","\n","        self.real_B_enc = util.encode_ab_ind(self.real_B[:, :, ::4, ::4], self.opt)\n","    \n","    def set_fusion_input(self, input, box_info):\n","        AtoB = self.opt.which_direction == 'AtoB'\n","        self.full_real_A = input['A' if AtoB else 'B'].to(self.device)\n","        self.full_real_B = input['B' if AtoB else 'A'].to(self.device)\n","\n","        self.full_hint_B = input['hint_B'].to(self.device)\n","        self.full_mask_B = input['mask_B'].to(self.device)\n","\n","        self.full_mask_B_nc = self.full_mask_B + self.opt.mask_cent\n","        self.full_real_B_enc = util.encode_ab_ind(self.full_real_B[:, :, ::4, ::4], self.opt)\n","        self.box_info_list = box_info\n","\n","    def forward(self):\n","        if self.opt.stage == 'full' or self.opt.stage == 'instance':\n","            (_, self.fake_B_reg) = self.netG(self.real_A, self.hint_B, self.mask_B)\n","        elif self.opt.stage == 'fusion':\n","            (_, self.comp_B_reg) = self.netGComp(self.full_real_A, self.full_hint_B, self.full_mask_B)\n","            (_, feature_map) = self.netG(self.real_A, self.hint_B, self.mask_B)\n","            self.fake_B_reg = self.netGF(self.full_real_A, self.full_hint_B, self.full_mask_B, feature_map, self.box_info_list)\n","        else:\n","            print('Error! Wrong stage selection!')\n","            exit()\n","\n","    def optimize_parameters(self):\n","        self.forward()\n","        self.optimizer_G.zero_grad()\n","        if self.opt.stage == 'full' or self.opt.stage == 'instance':\n","            self.loss_L1 = torch.mean(self.criterionL1(self.fake_B_reg.type(torch.cuda.FloatTensor),\n","                                                        self.real_B.type(torch.cuda.FloatTensor)))\n","            self.loss_G = 10 * torch.mean(self.criterionL1(self.fake_B_reg.type(torch.cuda.FloatTensor),\n","                                                        self.real_B.type(torch.cuda.FloatTensor)))\n","        elif self.opt.stage == 'fusion':\n","            self.loss_L1 = torch.mean(self.criterionL1(self.fake_B_reg.type(torch.cuda.FloatTensor),\n","                                                        self.full_real_B.type(torch.cuda.FloatTensor)))\n","            self.loss_G = 10 * torch.mean(self.criterionL1(self.fake_B_reg.type(torch.cuda.FloatTensor),\n","                                                        self.full_real_B.type(torch.cuda.FloatTensor)))\n","        else:\n","            print('Error! Wrong stage selection!')\n","            exit()\n","        self.loss_G.backward()\n","        self.optimizer_G.step()\n","\n","    def get_current_visuals(self):\n","        from collections import OrderedDict\n","        visual_ret = OrderedDict()\n","        if self.opt.stage == 'full' or self.opt.stage == 'instance':\n","            visual_ret['gray'] = util.lab2rgb(torch.cat((self.real_A.type(torch.cuda.FloatTensor), torch.zeros_like(self.real_B).type(torch.cuda.FloatTensor)), dim=1), self.opt)\n","            visual_ret['real'] = util.lab2rgb(torch.cat((self.real_A.type(torch.cuda.FloatTensor), self.real_B.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n","            visual_ret['fake_reg'] = util.lab2rgb(torch.cat((self.real_A.type(torch.cuda.FloatTensor), self.fake_B_reg.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n","\n","            visual_ret['hint'] = util.lab2rgb(torch.cat((self.real_A.type(torch.cuda.FloatTensor), self.hint_B.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n","            visual_ret['real_ab'] = util.lab2rgb(torch.cat((torch.zeros_like(self.real_A.type(torch.cuda.FloatTensor)), self.real_B.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n","            visual_ret['fake_ab_reg'] = util.lab2rgb(torch.cat((torch.zeros_like(self.real_A.type(torch.cuda.FloatTensor)), self.fake_B_reg.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n","            \n","        elif self.opt.stage == 'fusion':\n","            visual_ret['gray'] = util.lab2rgb(torch.cat((self.full_real_A.type(torch.cuda.FloatTensor), torch.zeros_like(self.full_real_B).type(torch.cuda.FloatTensor)), dim=1), self.opt)\n","            visual_ret['real'] = util.lab2rgb(torch.cat((self.full_real_A.type(torch.cuda.FloatTensor), self.full_real_B.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n","            visual_ret['comp_reg'] = util.lab2rgb(torch.cat((self.full_real_A.type(torch.cuda.FloatTensor), self.comp_B_reg.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n","            visual_ret['fake_reg'] = util.lab2rgb(torch.cat((self.full_real_A.type(torch.cuda.FloatTensor), self.fake_B_reg.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n","\n","            self.instance_mask = torch.nn.functional.interpolate(torch.zeros([1, 1, 176, 176]), size=visual_ret['gray'].shape[2:], mode='bilinear').type(torch.cuda.FloatTensor)\n","            visual_ret['box_mask'] = torch.cat((self.instance_mask, self.instance_mask, self.instance_mask), 1)\n","            visual_ret['real_ab'] = util.lab2rgb(torch.cat((torch.zeros_like(self.full_real_A.type(torch.cuda.FloatTensor)), self.full_real_B.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n","            visual_ret['comp_ab_reg'] = util.lab2rgb(torch.cat((torch.zeros_like(self.full_real_A.type(torch.cuda.FloatTensor)), self.comp_B_reg.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n","            visual_ret['fake_ab_reg'] = util.lab2rgb(torch.cat((torch.zeros_like(self.full_real_A.type(torch.cuda.FloatTensor)), self.fake_B_reg.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n","        else:\n","            print('Error! Wrong stage selection!')\n","            exit()\n","        return visual_ret\n","\n","    # return training losses/errors. train.py will print out these errors as debugging information\n","    def get_current_losses(self):\n","        self.error_cnt += 1\n","        errors_ret = OrderedDict()\n","        for name in self.loss_names:\n","            if isinstance(name, str):\n","                # float(...) works for both scalar tensor and float number\n","                self.avg_losses[name] = float(getattr(self, 'loss_' + name)) + self.avg_loss_alpha * self.avg_losses[name]\n","                errors_ret[name] = (1 - self.avg_loss_alpha) / (1 - self.avg_loss_alpha**self.error_cnt) * self.avg_losses[name]\n","        return errors_ret\n","\n","    def save_fusion_epoch(self, epoch):\n","        path = '{0}/{1}_net_GF.pth'.format(os.path.join(self.opt.checkpoints_dir, self.opt.name), epoch)\n","        latest_path = '{0}/latest_net_GF.pth'.format(os.path.join(self.opt.checkpoints_dir, self.opt.name))\n","        torch.save(self.netGF.state_dict(), path)\n","        torch.save(self.netGF.state_dict(), latest_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6fomf-BVLSTN"},"source":["4. Networks"]},{"cell_type":"code","metadata":{"id":"2pDhazJQLT-9"},"source":["import torch\n","import torch.nn as nn\n","from torch.nn import init\n","import functools\n","import torch.nn.functional as F\n","from torch.optim import lr_scheduler\n","\n","\n","def get_norm_layer(norm_type='instance'):\n","    if norm_type == 'batch':\n","        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n","    elif norm_type == 'instance':\n","        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n","    elif norm_type == 'none':\n","        norm_layer = None\n","    else:\n","        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n","    return norm_layer\n","\n","\n","def get_scheduler(optimizer, opt):\n","    if opt.lr_policy == 'lambda':\n","        def lambda_rule(epoch):\n","            lr_l = 1.0 - max(0, epoch + 1 + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n","            return lr_l\n","        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n","    elif opt.lr_policy == 'step':\n","        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n","    elif opt.lr_policy == 'plateau':\n","        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n","    else:\n","        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n","    return scheduler\n","\n","\n","def init_weights(net, init_type='xavier', gain=0.02):\n","    def init_func(m):\n","        classname = m.__class__.__name__\n","        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n","            if init_type == 'normal':\n","                init.normal_(m.weight.data, 0.0, gain)\n","            elif init_type == 'xavier':\n","                init.xavier_normal_(m.weight.data, gain=gain)\n","            elif init_type == 'kaiming':\n","                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n","            elif init_type == 'orthogonal':\n","                init.orthogonal_(m.weight.data, gain=gain)\n","            else:\n","                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n","            if hasattr(m, 'bias') and m.bias is not None:\n","                init.constant_(m.bias.data, 0.0)\n","        elif classname.find('BatchNorm2d') != -1:\n","            init.normal_(m.weight.data, 1.0, gain)\n","            init.constant_(m.bias.data, 0.0)\n","\n","    print('initialize network with %s' % init_type)\n","    net.apply(init_func)\n","\n","\n","def init_net(net, init_type='xavier', gpu_ids=[]):\n","    if len(gpu_ids) > 0:\n","        assert(torch.cuda.is_available())\n","        net.to(gpu_ids[0])\n","        net = torch.nn.DataParallel(net, gpu_ids)\n","    init_weights(net, init_type)\n","    return net\n","\n","\n","def define_G(input_nc, output_nc, ngf, which_model_netG, norm='batch', use_dropout=False, init_type='xavier', gpu_ids=[], use_tanh=True, classification=True):\n","    netG = None\n","    norm_layer = get_norm_layer(norm_type=norm)\n","\n","    if which_model_netG =='siggraph':\n","        netG = SIGGRAPHGenerator(input_nc, output_nc, norm_layer=norm_layer, use_tanh=use_tanh, classification=classification)\n","    elif which_model_netG =='instance':\n","        netG = InstanceGenerator(input_nc, output_nc, norm_layer=norm_layer, use_tanh=use_tanh, classification=classification)\n","    elif which_model_netG == 'fusion':\n","        netG = FusionGenerator(input_nc, output_nc, norm_layer=norm_layer, use_tanh=use_tanh, classification=classification)\n","    else:\n","        raise NotImplementedError('Generator model name [%s] is not recognized' % which_model_netG)\n","    return init_net(netG, init_type, gpu_ids)\n","\n","\n","class HuberLoss(nn.Module):\n","    def __init__(self, delta=.01):\n","        super(HuberLoss, self).__init__()\n","        self.delta=delta\n","\n","    def __call__(self, in0, in1):\n","        mask = torch.zeros_like(in0)\n","        mann = torch.abs(in0-in1)\n","        eucl = .5 * (mann**2)\n","        mask[...] = mann < self.delta\n","\n","        # loss = eucl*mask + self.delta*(mann-.5*self.delta)*(1-mask)\n","        loss = eucl*mask/self.delta + (mann-.5*self.delta)*(1-mask)\n","        return torch.sum(loss,dim=1,keepdim=True)\n","\n","\n","class L1Loss(nn.Module):\n","    def __init__(self):\n","        super(L1Loss, self).__init__()\n","\n","    def __call__(self, in0, in1):\n","        return torch.sum(torch.abs(in0-in1),dim=1,keepdim=True)\n","\n","\n","class SIGGRAPHGenerator(nn.Module):\n","    def __init__(self, input_nc, output_nc, norm_layer=nn.BatchNorm2d, use_tanh=True, classification=True):\n","        super(SIGGRAPHGenerator, self).__init__()\n","        self.input_nc = input_nc\n","        self.output_nc = output_nc\n","        self.classification = classification\n","        use_bias = True\n","\n","        # Conv1\n","        # model1=[nn.ReflectionPad2d(1),]\n","        model1=[nn.Conv2d(input_nc, 64, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model1+=[norm_layer(64),]\n","        model1+=[nn.ReLU(True),]\n","        # model1+=[nn.ReflectionPad2d(1),]\n","        model1+=[nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model1+=[nn.ReLU(True),]\n","        model1+=[norm_layer(64),]\n","        # add a subsampling operation\n","\n","        # Conv2\n","        # model2=[nn.ReflectionPad2d(1),]\n","        model2=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model2+=[norm_layer(128),]\n","        model2+=[nn.ReLU(True),]\n","        # model2+=[nn.ReflectionPad2d(1),]\n","        model2+=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model2+=[nn.ReLU(True),]\n","        model2+=[norm_layer(128),]\n","        # add a subsampling layer operation\n","\n","        # Conv3\n","        # model3=[nn.ReflectionPad2d(1),]\n","        model3=[nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model3+=[norm_layer(256),]\n","        model3+=[nn.ReLU(True),]\n","        # model3+=[nn.ReflectionPad2d(1),]\n","        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model3+=[norm_layer(256),]\n","        model3+=[nn.ReLU(True),]\n","        # model3+=[nn.ReflectionPad2d(1),]\n","        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model3+=[nn.ReLU(True),]\n","        model3+=[norm_layer(256),]\n","        # add a subsampling layer operation\n","\n","        # Conv4\n","        # model47=[nn.ReflectionPad2d(1),]\n","        model4=[nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model4+=[norm_layer(512),]\n","        model4+=[nn.ReLU(True),]\n","        # model4+=[nn.ReflectionPad2d(1),]\n","        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model4+=[norm_layer(512),]\n","        model4+=[nn.ReLU(True),]\n","        # model4+=[nn.ReflectionPad2d(1),]\n","        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model4+=[nn.ReLU(True),]\n","        model4+=[norm_layer(512),]\n","\n","        # Conv5\n","        # model47+=[nn.ReflectionPad2d(2),]\n","        model5=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        # model5+=[norm_layer(512),]\n","        model5+=[nn.ReLU(True),]\n","        # model5+=[nn.ReflectionPad2d(2),]\n","        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        # model5+=[norm_layer(512),]\n","        model5+=[nn.ReLU(True),]\n","        # model5+=[nn.ReflectionPad2d(2),]\n","        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        model5+=[nn.ReLU(True),]\n","        model5+=[norm_layer(512),]\n","\n","        # Conv6\n","        # model6+=[nn.ReflectionPad2d(2),]\n","        model6=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        # model6+=[norm_layer(512),]\n","        model6+=[nn.ReLU(True),]\n","        # model6+=[nn.ReflectionPad2d(2),]\n","        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        # model6+=[norm_layer(512),]\n","        model6+=[nn.ReLU(True),]\n","        # model6+=[nn.ReflectionPad2d(2),]\n","        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        model6+=[nn.ReLU(True),]\n","        model6+=[norm_layer(512),]\n","\n","        # Conv7\n","        # model47+=[nn.ReflectionPad2d(1),]\n","        model7=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model7+=[norm_layer(512),]\n","        model7+=[nn.ReLU(True),]\n","        # model7+=[nn.ReflectionPad2d(1),]\n","        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model7+=[norm_layer(512),]\n","        model7+=[nn.ReLU(True),]\n","        # model7+=[nn.ReflectionPad2d(1),]\n","        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model7+=[nn.ReLU(True),]\n","        model7+=[norm_layer(512),]\n","\n","        # Conv7\n","        model8up=[nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=use_bias)]\n","\n","        # model3short8=[nn.ReflectionPad2d(1),]\n","        model3short8=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","\n","        # model47+=[norm_layer(256),]\n","        model8=[nn.ReLU(True),]\n","        # model8+=[nn.ReflectionPad2d(1),]\n","        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model8+=[norm_layer(256),]\n","        model8+=[nn.ReLU(True),]\n","        # model8+=[nn.ReflectionPad2d(1),]\n","        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model8+=[nn.ReLU(True),]\n","        model8+=[norm_layer(256),]\n","\n","        # Conv9\n","        model9up=[nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=use_bias),]\n","\n","        # model2short9=[nn.ReflectionPad2d(1),]\n","        model2short9=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # add the two feature maps above        \n","\n","        # model9=[norm_layer(128),]\n","        model9=[nn.ReLU(True),]\n","        # model9+=[nn.ReflectionPad2d(1),]\n","        model9+=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model9+=[nn.ReLU(True),]\n","        model9+=[norm_layer(128),]\n","\n","        # Conv10\n","        model10up=[nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1, bias=use_bias),]\n","\n","        # model1short10=[nn.ReflectionPad2d(1),]\n","        model1short10=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # add the two feature maps above\n","\n","        # model10=[norm_layer(128),]\n","        model10=[nn.ReLU(True),]\n","        # model10+=[nn.ReflectionPad2d(1),]\n","        model10+=[nn.Conv2d(128, 128, kernel_size=3, dilation=1, stride=1, padding=1, bias=use_bias),]\n","        model10+=[nn.LeakyReLU(negative_slope=.2),]\n","\n","        # classification output\n","        model_class=[nn.Conv2d(256, 529, kernel_size=1, padding=0, dilation=1, stride=1, bias=use_bias),]\n","\n","        # regression output\n","        model_out=[nn.Conv2d(128, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=use_bias),]\n","        if(use_tanh):\n","            model_out+=[nn.Tanh()]\n","\n","        self.model1 = nn.Sequential(*model1)\n","        self.model2 = nn.Sequential(*model2)\n","        self.model3 = nn.Sequential(*model3)\n","        self.model4 = nn.Sequential(*model4)\n","        self.model5 = nn.Sequential(*model5)\n","        self.model6 = nn.Sequential(*model6)\n","        self.model7 = nn.Sequential(*model7)\n","        self.model8up = nn.Sequential(*model8up)\n","        self.model8 = nn.Sequential(*model8)\n","        self.model9up = nn.Sequential(*model9up)\n","        self.model9 = nn.Sequential(*model9)\n","        self.model10up = nn.Sequential(*model10up)\n","        self.model10 = nn.Sequential(*model10)\n","        self.model3short8 = nn.Sequential(*model3short8)\n","        self.model2short9 = nn.Sequential(*model2short9)\n","        self.model1short10 = nn.Sequential(*model1short10)\n","\n","        self.model_class = nn.Sequential(*model_class)\n","        self.model_out = nn.Sequential(*model_out)\n","\n","        self.upsample4 = nn.Sequential(*[nn.Upsample(scale_factor=4, mode='nearest'),])\n","        self.softmax = nn.Sequential(*[nn.Softmax(dim=1),])\n","\n","    def forward(self, input_A, input_B, mask_B):\n","        conv1_2 = self.model1(torch.cat((input_A,input_B,mask_B),dim=1))\n","        conv2_2 = self.model2(conv1_2[:,:,::2,::2])\n","        conv3_3 = self.model3(conv2_2[:,:,::2,::2])\n","        conv4_3 = self.model4(conv3_3[:,:,::2,::2])\n","        conv5_3 = self.model5(conv4_3)\n","        conv6_3 = self.model6(conv5_3)\n","        conv7_3 = self.model7(conv6_3)\n","        conv8_up = self.model8up(conv7_3) + self.model3short8(conv3_3)\n","        conv8_3 = self.model8(conv8_up)\n","\n","        if(self.classification):\n","            out_class = self.model_class(conv8_3)\n","            conv9_up = self.model9up(conv8_3.detach()) + self.model2short9(conv2_2.detach())\n","            conv9_3 = self.model9(conv9_up)\n","            conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2.detach())\n","            conv10_2 = self.model10(conv10_up)\n","            out_reg = self.model_out(conv10_2)\n","        else:\n","            out_class = self.model_class(conv8_3.detach())\n","\n","            conv9_up = self.model9up(conv8_3) + self.model2short9(conv2_2)\n","            conv9_3 = self.model9(conv9_up)\n","            conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2)\n","            conv10_2 = self.model10(conv10_up)\n","            out_reg = self.model_out(conv10_2)\n","\n","        return (out_class, out_reg)\n","\n","\n","class FusionGenerator(nn.Module):\n","    def __init__(self, input_nc, output_nc, norm_layer=nn.BatchNorm2d, use_tanh=True, classification=True):\n","        super(FusionGenerator, self).__init__()\n","        self.input_nc = input_nc\n","        self.output_nc = output_nc\n","        self.classification = classification\n","        use_bias = True\n","\n","        # Conv1\n","        # model1=[nn.ReflectionPad2d(1),]\n","        model1=[nn.Conv2d(input_nc, 64, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model1+=[norm_layer(64),]\n","        model1+=[nn.ReLU(True),]\n","        # model1+=[nn.ReflectionPad2d(1),]\n","        model1+=[nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model1+=[nn.ReLU(True),]\n","        model1+=[norm_layer(64),]\n","        # add a subsampling operation\n","\n","        self.weight_layer = WeightGenerator(64)\n","        \n","        # Conv2\n","        # model2=[nn.ReflectionPad2d(1),]\n","        model2=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model2+=[norm_layer(128),]\n","        model2+=[nn.ReLU(True),]\n","        # model2+=[nn.ReflectionPad2d(1),]\n","        model2+=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model2+=[nn.ReLU(True),]\n","        model2+=[norm_layer(128),]\n","        # add a subsampling layer operation\n","\n","        self.weight_layer2 = WeightGenerator(128)\n","\n","        # Conv3\n","        # model3=[nn.ReflectionPad2d(1),]\n","        model3=[nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model3+=[norm_layer(256),]\n","        model3+=[nn.ReLU(True),]\n","        # model3+=[nn.ReflectionPad2d(1),]\n","        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model3+=[norm_layer(256),]\n","        model3+=[nn.ReLU(True),]\n","        # model3+=[nn.ReflectionPad2d(1),]\n","        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model3+=[nn.ReLU(True),]\n","        model3+=[norm_layer(256),]\n","        # add a subsampling layer operation\n","\n","        self.weight_layer3 = WeightGenerator(256)\n","\n","        # Conv4\n","        # model47=[nn.ReflectionPad2d(1),]\n","        model4=[nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model4+=[norm_layer(512),]\n","        model4+=[nn.ReLU(True),]\n","        # model4+=[nn.ReflectionPad2d(1),]\n","        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model4+=[norm_layer(512),]\n","        model4+=[nn.ReLU(True),]\n","        # model4+=[nn.ReflectionPad2d(1),]\n","        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model4+=[nn.ReLU(True),]\n","        model4+=[norm_layer(512),]\n","\n","        self.weight_layer4 = WeightGenerator(512)\n","\n","        # Conv5\n","        # model47+=[nn.ReflectionPad2d(2),]\n","        model5=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        # model5+=[norm_layer(512),]\n","        model5+=[nn.ReLU(True),]\n","        # model5+=[nn.ReflectionPad2d(2),]\n","        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        # model5+=[norm_layer(512),]\n","        model5+=[nn.ReLU(True),]\n","        # model5+=[nn.ReflectionPad2d(2),]\n","        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        model5+=[nn.ReLU(True),]\n","        model5+=[norm_layer(512),]\n","\n","        self.weight_layer5 = WeightGenerator(512)\n","\n","        # Conv6\n","        # model6+=[nn.ReflectionPad2d(2),]\n","        model6=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        # model6+=[norm_layer(512),]\n","        model6+=[nn.ReLU(True),]\n","        # model6+=[nn.ReflectionPad2d(2),]\n","        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        # model6+=[norm_layer(512),]\n","        model6+=[nn.ReLU(True),]\n","        # model6+=[nn.ReflectionPad2d(2),]\n","        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        model6+=[nn.ReLU(True),]\n","        model6+=[norm_layer(512),]\n","\n","        self.weight_layer6 = WeightGenerator(512)\n","\n","        # Conv7\n","        # model47+=[nn.ReflectionPad2d(1),]\n","        model7=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model7+=[norm_layer(512),]\n","        model7+=[nn.ReLU(True),]\n","        # model7+=[nn.ReflectionPad2d(1),]\n","        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model7+=[norm_layer(512),]\n","        model7+=[nn.ReLU(True),]\n","        # model7+=[nn.ReflectionPad2d(1),]\n","        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model7+=[nn.ReLU(True),]\n","        model7+=[norm_layer(512),]\n","\n","        self.weight_layer7 = WeightGenerator(512)\n","\n","        # Conv7\n","        model8up=[nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=use_bias)]\n","\n","        # model3short8=[nn.ReflectionPad2d(1),]\n","        model3short8=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","\n","        self.weight_layer8_1 = WeightGenerator(256)\n","\n","        # model47+=[norm_layer(256),]\n","        model8=[nn.ReLU(True),]\n","        # model8+=[nn.ReflectionPad2d(1),]\n","        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model8+=[norm_layer(256),]\n","        model8+=[nn.ReLU(True),]\n","        # model8+=[nn.ReflectionPad2d(1),]\n","        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model8+=[nn.ReLU(True),]\n","        model8+=[norm_layer(256),]\n","\n","        self.weight_layer8_2 = WeightGenerator(256)\n","\n","        # Conv9\n","        model9up=[nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=use_bias),]\n","\n","        # model2short9=[nn.ReflectionPad2d(1),]\n","        model2short9=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # add the two feature maps above        \n","\n","        self.weight_layer9_1 = WeightGenerator(128)\n","\n","        # model9=[norm_layer(128),]\n","        model9=[nn.ReLU(True),]\n","        # model9+=[nn.ReflectionPad2d(1),]\n","        model9+=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model9+=[nn.ReLU(True),]\n","        model9+=[norm_layer(128),]\n","\n","        self.weight_layer9_2 = WeightGenerator(128)\n","\n","        # Conv10\n","        model10up=[nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1, bias=use_bias),]\n","\n","        # model1short10=[nn.ReflectionPad2d(1),]\n","        model1short10=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # add the two feature maps above\n","\n","        self.weight_layer10_1 = WeightGenerator(128)\n","\n","        # model10=[norm_layer(128),]\n","        model10=[nn.ReLU(True),]\n","        # model10+=[nn.ReflectionPad2d(1),]\n","        model10+=[nn.Conv2d(128, 128, kernel_size=3, dilation=1, stride=1, padding=1, bias=use_bias),]\n","        model10+=[nn.LeakyReLU(negative_slope=.2),]\n","\n","        self.weight_layer10_2 = WeightGenerator(128)\n","\n","        # classification output\n","        model_class=[nn.Conv2d(256, 529, kernel_size=1, padding=0, dilation=1, stride=1, bias=use_bias),]\n","\n","        # regression output\n","        model_out=[nn.Conv2d(128, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=use_bias),]\n","        if(use_tanh):\n","            model_out+=[nn.Tanh()]\n","\n","        self.weight_layerout = WeightGenerator(2)\n","\n","        self.model1 = nn.Sequential(*model1)\n","        self.model2 = nn.Sequential(*model2)\n","        self.model3 = nn.Sequential(*model3)\n","        self.model4 = nn.Sequential(*model4)\n","        self.model5 = nn.Sequential(*model5)\n","        self.model6 = nn.Sequential(*model6)\n","        self.model7 = nn.Sequential(*model7)\n","        self.model8up = nn.Sequential(*model8up)\n","        self.model8 = nn.Sequential(*model8)\n","        self.model9up = nn.Sequential(*model9up)\n","        self.model9 = nn.Sequential(*model9)\n","        self.model10up = nn.Sequential(*model10up)\n","        self.model10 = nn.Sequential(*model10)\n","        self.model3short8 = nn.Sequential(*model3short8)\n","        self.model2short9 = nn.Sequential(*model2short9)\n","        self.model1short10 = nn.Sequential(*model1short10)\n","\n","        self.model_class = nn.Sequential(*model_class)\n","        self.model_out = nn.Sequential(*model_out)\n","\n","        self.upsample4 = nn.Sequential(*[nn.Upsample(scale_factor=4, mode='nearest'),])\n","        self.softmax = nn.Sequential(*[nn.Softmax(dim=1),])\n","\n","    def forward(self, input_A, input_B, mask_B, instance_feature, box_info_list):\n","        conv1_2 = self.model1(torch.cat((input_A,input_B,mask_B),dim=1))\n","        conv1_2 = self.weight_layer(instance_feature['conv1_2'], conv1_2, box_info_list[0])\n","\n","        conv2_2 = self.model2(conv1_2[:,:,::2,::2])\n","        conv2_2 = self.weight_layer2(instance_feature['conv2_2'], conv2_2, box_info_list[1])\n","\n","        conv3_3 = self.model3(conv2_2[:,:,::2,::2])\n","        conv3_3 = self.weight_layer3(instance_feature['conv3_3'], conv3_3, box_info_list[2])\n","\n","        conv4_3 = self.model4(conv3_3[:,:,::2,::2])\n","        conv4_3 = self.weight_layer4(instance_feature['conv4_3'], conv4_3, box_info_list[3])\n","\n","        conv5_3 = self.model5(conv4_3)\n","        conv5_3 = self.weight_layer5(instance_feature['conv5_3'], conv5_3, box_info_list[3])\n","\n","        conv6_3 = self.model6(conv5_3)\n","        conv6_3 = self.weight_layer6(instance_feature['conv6_3'], conv6_3, box_info_list[3])\n","\n","        conv7_3 = self.model7(conv6_3)\n","        conv7_3 = self.weight_layer7(instance_feature['conv7_3'], conv7_3, box_info_list[3])\n","\n","        conv8_up = self.model8up(conv7_3) + self.model3short8(conv3_3)\n","        conv8_up = self.weight_layer8_1(instance_feature['conv8_up'], conv8_up, box_info_list[2])\n","\n","        conv8_3 = self.model8(conv8_up)\n","        conv8_3 = self.weight_layer8_2(instance_feature['conv8_3'], conv8_3, box_info_list[2])\n","\n","        conv9_up = self.model9up(conv8_3) + self.model2short9(conv2_2)\n","        conv9_up = self.weight_layer9_1(instance_feature['conv9_up'], conv9_up, box_info_list[1])\n","\n","        conv9_3 = self.model9(conv9_up)\n","        conv9_3 = self.weight_layer9_2(instance_feature['conv9_3'], conv9_3, box_info_list[1])\n","\n","        conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2)\n","        conv10_up = self.weight_layer10_1(instance_feature['conv10_up'], conv10_up, box_info_list[0])\n","\n","        conv10_2 = self.model10(conv10_up)\n","        conv10_2 = self.weight_layer10_2(instance_feature['conv10_2'], conv10_2, box_info_list[0])\n","        \n","        out_reg = self.model_out(conv10_2)\n","        return out_reg\n","\n","\n","class WeightGenerator(nn.Module):\n","    def __init__(self, input_ch, inner_ch=16):\n","        super(WeightGenerator, self).__init__()\n","        self.simple_instance_conv = nn.Sequential(\n","            nn.Conv2d(input_ch, inner_ch, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(True),\n","            nn.Conv2d(inner_ch, inner_ch, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(True),\n","            nn.Conv2d(inner_ch, 1, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(True),\n","        )\n","\n","        self.simple_bg_conv = nn.Sequential(\n","            nn.Conv2d(input_ch, inner_ch, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(True),\n","            nn.Conv2d(inner_ch, inner_ch, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(True),\n","            nn.Conv2d(inner_ch, 1, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(True),\n","        )\n","\n","        self.normalize = nn.Softmax(1)\n","    \n","    def resize_and_pad(self, feauture_maps, info_array):\n","        feauture_maps = torch.nn.functional.interpolate(feauture_maps, size=(info_array[5], info_array[4]), mode='bilinear')\n","        feauture_maps = torch.nn.functional.pad(feauture_maps, (info_array[0], info_array[1], info_array[2], info_array[3]), \"constant\", 0)\n","        return feauture_maps\n","    \n","    def forward(self, instance_feature, bg_feature, box_info):\n","        mask_list = []\n","        featur_map_list = []\n","        mask_sum_for_pred = torch.zeros_like(bg_feature)[:1, :1]\n","        for i in range(instance_feature.shape[0]):\n","            tmp_crop = torch.unsqueeze(instance_feature[i], 0)\n","            conv_tmp_crop = self.simple_instance_conv(tmp_crop)\n","            pred_mask = self.resize_and_pad(conv_tmp_crop, box_info[i])\n","            \n","            tmp_crop = self.resize_and_pad(tmp_crop, box_info[i])\n","\n","            mask = torch.zeros_like(bg_feature)[:1, :1]\n","            mask[0, 0, box_info[i][2]:box_info[i][2] + box_info[i][5], box_info[i][0]:box_info[i][0] + box_info[i][4]] = 1.0\n","            device = mask.device\n","            mask = mask.type(torch.FloatTensor).to(device)\n","\n","            mask_sum_for_pred = torch.clamp(mask_sum_for_pred + mask, 0.0, 1.0)\n","\n","            mask_list.append(pred_mask)\n","            featur_map_list.append(tmp_crop)\n","\n","        pred_bg_mask = self.simple_bg_conv(bg_feature)\n","        mask_list.append(pred_bg_mask + (1 - mask_sum_for_pred) * 100000.0)\n","        mask_list = self.normalize(torch.cat(mask_list, 1))\n","\n","        mask_list_maskout = mask_list.clone()\n","        \n","        instance_mask = torch.clamp(torch.sum(mask_list_maskout[:, :instance_feature.shape[0]], 1, keepdim=True), 0.0, 1.0)\n","\n","        featur_map_list.append(bg_feature)\n","        featur_map_list = torch.cat(featur_map_list, 0)\n","        mask_list_maskout = mask_list_maskout.permute(1, 0, 2, 3).contiguous()\n","        out = featur_map_list * mask_list_maskout\n","        out = torch.sum(out, 0, keepdim=True)\n","        return out # , instance_mask, torch.clamp(mask_list, 0.0, 1.0)\n","\n","\n","class InstanceGenerator(nn.Module):\n","    def __init__(self, input_nc, output_nc, norm_layer=nn.BatchNorm2d, use_tanh=True, classification=True):\n","        super(InstanceGenerator, self).__init__()\n","        self.input_nc = input_nc\n","        self.output_nc = output_nc\n","        self.classification = classification\n","        use_bias = True\n","\n","        # Conv1\n","        # model1=[nn.ReflectionPad2d(1),]\n","        model1=[nn.Conv2d(input_nc, 64, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model1+=[norm_layer(64),]\n","        model1+=[nn.ReLU(True),]\n","        # model1+=[nn.ReflectionPad2d(1),]\n","        model1+=[nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model1+=[nn.ReLU(True),]\n","        model1+=[norm_layer(64),]\n","        # add a subsampling operation\n","\n","        # Conv2\n","        # model2=[nn.ReflectionPad2d(1),]\n","        model2=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model2+=[norm_layer(128),]\n","        model2+=[nn.ReLU(True),]\n","        # model2+=[nn.ReflectionPad2d(1),]\n","        model2+=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model2+=[nn.ReLU(True),]\n","        model2+=[norm_layer(128),]\n","        # add a subsampling layer operation\n","\n","        # Conv3\n","        # model3=[nn.ReflectionPad2d(1),]\n","        model3=[nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model3+=[norm_layer(256),]\n","        model3+=[nn.ReLU(True),]\n","        # model3+=[nn.ReflectionPad2d(1),]\n","        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model3+=[norm_layer(256),]\n","        model3+=[nn.ReLU(True),]\n","        # model3+=[nn.ReflectionPad2d(1),]\n","        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model3+=[nn.ReLU(True),]\n","        model3+=[norm_layer(256),]\n","        # add a subsampling layer operation\n","\n","        # Conv4\n","        # model47=[nn.ReflectionPad2d(1),]\n","        model4=[nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model4+=[norm_layer(512),]\n","        model4+=[nn.ReLU(True),]\n","        # model4+=[nn.ReflectionPad2d(1),]\n","        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model4+=[norm_layer(512),]\n","        model4+=[nn.ReLU(True),]\n","        # model4+=[nn.ReflectionPad2d(1),]\n","        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model4+=[nn.ReLU(True),]\n","        model4+=[norm_layer(512),]\n","\n","        # Conv5\n","        # model47+=[nn.ReflectionPad2d(2),]\n","        model5=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        # model5+=[norm_layer(512),]\n","        model5+=[nn.ReLU(True),]\n","        # model5+=[nn.ReflectionPad2d(2),]\n","        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        # model5+=[norm_layer(512),]\n","        model5+=[nn.ReLU(True),]\n","        # model5+=[nn.ReflectionPad2d(2),]\n","        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        model5+=[nn.ReLU(True),]\n","        model5+=[norm_layer(512),]\n","\n","        # Conv6\n","        # model6+=[nn.ReflectionPad2d(2),]\n","        model6=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        # model6+=[norm_layer(512),]\n","        model6+=[nn.ReLU(True),]\n","        # model6+=[nn.ReflectionPad2d(2),]\n","        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        # model6+=[norm_layer(512),]\n","        model6+=[nn.ReLU(True),]\n","        # model6+=[nn.ReflectionPad2d(2),]\n","        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n","        model6+=[nn.ReLU(True),]\n","        model6+=[norm_layer(512),]\n","\n","        # Conv7\n","        # model47+=[nn.ReflectionPad2d(1),]\n","        model7=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model7+=[norm_layer(512),]\n","        model7+=[nn.ReLU(True),]\n","        # model7+=[nn.ReflectionPad2d(1),]\n","        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model7+=[norm_layer(512),]\n","        model7+=[nn.ReLU(True),]\n","        # model7+=[nn.ReflectionPad2d(1),]\n","        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model7+=[nn.ReLU(True),]\n","        model7+=[norm_layer(512),]\n","\n","        # Conv7\n","        model8up=[nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=use_bias)]\n","\n","        # model3short8=[nn.ReflectionPad2d(1),]\n","        model3short8=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","\n","        # model47+=[norm_layer(256),]\n","        model8=[nn.ReLU(True),]\n","        # model8+=[nn.ReflectionPad2d(1),]\n","        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # model8+=[norm_layer(256),]\n","        model8+=[nn.ReLU(True),]\n","        # model8+=[nn.ReflectionPad2d(1),]\n","        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model8+=[nn.ReLU(True),]\n","        model8+=[norm_layer(256),]\n","\n","        # Conv9\n","        model9up=[nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=use_bias),]\n","\n","        # model2short9=[nn.ReflectionPad2d(1),]\n","        model2short9=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # add the two feature maps above        \n","\n","        # model9=[norm_layer(128),]\n","        model9=[nn.ReLU(True),]\n","        # model9+=[nn.ReflectionPad2d(1),]\n","        model9+=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        model9+=[nn.ReLU(True),]\n","        model9+=[norm_layer(128),]\n","\n","        # Conv10\n","        model10up=[nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1, bias=use_bias),]\n","\n","        # model1short10=[nn.ReflectionPad2d(1),]\n","        model1short10=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n","        # add the two feature maps above\n","\n","        # model10=[norm_layer(128),]\n","        model10=[nn.ReLU(True),]\n","        # model10+=[nn.ReflectionPad2d(1),]\n","        model10+=[nn.Conv2d(128, 128, kernel_size=3, dilation=1, stride=1, padding=1, bias=use_bias),]\n","        model10+=[nn.LeakyReLU(negative_slope=.2),]\n","\n","        # classification output\n","        model_class=[nn.Conv2d(256, 529, kernel_size=1, padding=0, dilation=1, stride=1, bias=use_bias),]\n","\n","        # regression output\n","        model_out=[nn.Conv2d(128, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=use_bias),]\n","        if(use_tanh):\n","            model_out+=[nn.Tanh()]\n","\n","        self.model1 = nn.Sequential(*model1)\n","        self.model2 = nn.Sequential(*model2)\n","        self.model3 = nn.Sequential(*model3)\n","        self.model4 = nn.Sequential(*model4)\n","        self.model5 = nn.Sequential(*model5)\n","        self.model6 = nn.Sequential(*model6)\n","        self.model7 = nn.Sequential(*model7)\n","        self.model8up = nn.Sequential(*model8up)\n","        self.model8 = nn.Sequential(*model8)\n","        self.model9up = nn.Sequential(*model9up)\n","        self.model9 = nn.Sequential(*model9)\n","        self.model10up = nn.Sequential(*model10up)\n","        self.model10 = nn.Sequential(*model10)\n","        self.model3short8 = nn.Sequential(*model3short8)\n","        self.model2short9 = nn.Sequential(*model2short9)\n","        self.model1short10 = nn.Sequential(*model1short10)\n","\n","        self.model_class = nn.Sequential(*model_class)\n","        self.model_out = nn.Sequential(*model_out)\n","\n","        self.upsample4 = nn.Sequential(*[nn.Upsample(scale_factor=4, mode='nearest'),])\n","        self.softmax = nn.Sequential(*[nn.Softmax(dim=1),])\n","\n","    def forward(self, input_A, input_B, mask_B):\n","        conv1_2 = self.model1(torch.cat((input_A,input_B,mask_B),dim=1))\n","        conv2_2 = self.model2(conv1_2[:,:,::2,::2])\n","        conv3_3 = self.model3(conv2_2[:,:,::2,::2])\n","        conv4_3 = self.model4(conv3_3[:,:,::2,::2])\n","        conv5_3 = self.model5(conv4_3)\n","        conv6_3 = self.model6(conv5_3)\n","        conv7_3 = self.model7(conv6_3)\n","        conv8_up = self.model8up(conv7_3) + self.model3short8(conv3_3)\n","        conv8_3 = self.model8(conv8_up)\n","\n","        if(self.classification):\n","            out_class = self.model_class(conv8_3)\n","            conv9_up = self.model9up(conv8_3.detach()) + self.model2short9(conv2_2.detach())\n","            conv9_3 = self.model9(conv9_up)\n","            conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2.detach())\n","            conv10_2 = self.model10(conv10_up)\n","            out_reg = self.model_out(conv10_2)\n","        else:\n","            out_class = self.model_class(conv8_3.detach())\n","\n","            conv9_up = self.model9up(conv8_3) + self.model2short9(conv2_2)\n","            conv9_3 = self.model9(conv9_up)\n","            conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2)\n","            conv10_2 = self.model10(conv10_up)\n","            out_reg = self.model_out(conv10_2)\n","\n","        feature_map = {}\n","        feature_map['conv1_2'] = conv1_2\n","        feature_map['conv2_2'] = conv2_2\n","        feature_map['conv3_3'] = conv3_3\n","        feature_map['conv4_3'] = conv4_3\n","        feature_map['conv5_3'] = conv5_3\n","        feature_map['conv6_3'] = conv6_3\n","        feature_map['conv7_3'] = conv7_3\n","        feature_map['conv8_up'] = conv8_up\n","        feature_map['conv8_3'] = conv8_3\n","        feature_map['conv9_up'] = conv9_up\n","        feature_map['conv9_3'] = conv9_3\n","        feature_map['conv10_up'] = conv10_up\n","        feature_map['conv10_2'] = conv10_2\n","        feature_map['out_reg'] = out_reg\n","        return (out_reg, feature_map)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6idsRYPzOWPC"},"source":["# Utils"]},{"cell_type":"markdown","metadata":{"id":"cgIn-viHOZTr"},"source":["util"]},{"cell_type":"code","metadata":{"id":"JIs-HxHAOjOf"},"source":["from __future__ import print_function\n","import torch\n","import numpy as np\n","from PIL import Image\n","import os\n","from collections import OrderedDict\n","from IPython import embed\n","import cv2\n","\n","# Converts a Tensor into an image array (numpy)\n","# |imtype|: the desired type of the converted numpy array\n","def tensor2im(input_image, imtype=np.uint8):\n","    if isinstance(input_image, torch.Tensor):\n","        image_tensor = input_image.data\n","    else:\n","        return input_image\n","    image_numpy = image_tensor[0].cpu().float().numpy()\n","    if image_numpy.shape[0] == 1:\n","        image_numpy = np.tile(image_numpy, (3, 1, 1))\n","    image_numpy = np.clip((np.transpose(image_numpy, (1, 2, 0)) ),0, 1) * 255.0\n","    return image_numpy.astype(imtype)\n","\n","\n","def diagnose_network(net, name='network'):\n","    mean = 0.0\n","    count = 0\n","    for param in net.parameters():\n","        if param.grad is not None:\n","            mean += torch.mean(torch.abs(param.grad.data))\n","            count += 1\n","    if count > 0:\n","        mean = mean / count\n","    print(name)\n","    print(mean)\n","\n","\n","def save_image(image_numpy, image_path):\n","    image_pil = Image.fromarray(image_numpy)\n","    image_pil.save(image_path)\n","\n","\n","def print_numpy(x, val=True, shp=False):\n","    x = x.astype(np.float64)\n","    if shp:\n","        print('shape,', x.shape)\n","    if val:\n","        x = x.flatten()\n","        print('mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f' % (\n","            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n","\n","\n","def mkdirs(paths):\n","    if isinstance(paths, list) and not isinstance(paths, str):\n","        for path in paths:\n","            mkdir(path)\n","    else:\n","        mkdir(paths)\n","\n","\n","def mkdir(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","\n","def get_subset_dict(in_dict,keys):\n","    if(len(keys)):\n","        subset = OrderedDict()\n","        for key in keys:\n","            subset[key] = in_dict[key]\n","    else:\n","        subset = in_dict\n","    return subset\n","\n","\n","\n","# Color conversion code\n","def rgb2xyz(rgb): # rgb from [0,1]\n","    # xyz_from_rgb = np.array([[0.412453, 0.357580, 0.180423],\n","        # [0.212671, 0.715160, 0.072169],\n","        # [0.019334, 0.119193, 0.950227]])\n","\n","    mask = (rgb > .04045).type(torch.FloatTensor)\n","    if(rgb.is_cuda):\n","        mask = mask.cuda()\n","\n","    rgb = (((rgb+.055)/1.055)**2.4)*mask + rgb/12.92*(1-mask)\n","\n","    x = .412453*rgb[:,0,:,:]+.357580*rgb[:,1,:,:]+.180423*rgb[:,2,:,:]\n","    y = .212671*rgb[:,0,:,:]+.715160*rgb[:,1,:,:]+.072169*rgb[:,2,:,:]\n","    z = .019334*rgb[:,0,:,:]+.119193*rgb[:,1,:,:]+.950227*rgb[:,2,:,:]\n","    out = torch.cat((x[:,None,:,:],y[:,None,:,:],z[:,None,:,:]),dim=1)\n","\n","    # if(torch.sum(torch.isnan(out))>0):\n","        # print('rgb2xyz')\n","        # embed()\n","    return out\n","\n","def xyz2rgb(xyz):\n","    # array([[ 3.24048134, -1.53715152, -0.49853633],\n","    #        [-0.96925495,  1.87599   ,  0.04155593],\n","    #        [ 0.05564664, -0.20404134,  1.05731107]])\n","\n","    r = 3.24048134*xyz[:,0,:,:]-1.53715152*xyz[:,1,:,:]-0.49853633*xyz[:,2,:,:]\n","    g = -0.96925495*xyz[:,0,:,:]+1.87599*xyz[:,1,:,:]+.04155593*xyz[:,2,:,:]\n","    b = .05564664*xyz[:,0,:,:]-.20404134*xyz[:,1,:,:]+1.05731107*xyz[:,2,:,:]\n","\n","    rgb = torch.cat((r[:,None,:,:],g[:,None,:,:],b[:,None,:,:]),dim=1)\n","    rgb = torch.max(rgb,torch.zeros_like(rgb)) # sometimes reaches a small negative number, which causes NaNs\n","\n","    mask = (rgb > .0031308).type(torch.FloatTensor)\n","    if(rgb.is_cuda):\n","        mask = mask.cuda()\n","\n","    rgb = (1.055*(rgb**(1./2.4)) - 0.055)*mask + 12.92*rgb*(1-mask)\n","\n","    # if(torch.sum(torch.isnan(rgb))>0):\n","        # print('xyz2rgb')\n","        # embed()\n","    return rgb\n","\n","def xyz2lab(xyz):\n","    # 0.95047, 1., 1.08883 # white\n","    sc = torch.Tensor((0.95047, 1., 1.08883))[None,:,None,None]\n","    if(xyz.is_cuda):\n","        sc = sc.cuda()\n","\n","    xyz_scale = xyz/sc\n","\n","    mask = (xyz_scale > .008856).type(torch.FloatTensor)\n","    if(xyz_scale.is_cuda):\n","        mask = mask.cuda()\n","\n","    xyz_int = xyz_scale**(1/3.)*mask + (7.787*xyz_scale + 16./116.)*(1-mask)\n","\n","    L = 116.*xyz_int[:,1,:,:]-16.\n","    a = 500.*(xyz_int[:,0,:,:]-xyz_int[:,1,:,:])\n","    b = 200.*(xyz_int[:,1,:,:]-xyz_int[:,2,:,:])\n","    out = torch.cat((L[:,None,:,:],a[:,None,:,:],b[:,None,:,:]),dim=1)\n","\n","    # if(torch.sum(torch.isnan(out))>0):\n","        # print('xyz2lab')\n","        # embed()\n","\n","    return out\n","\n","def lab2xyz(lab):\n","    y_int = (lab[:,0,:,:]+16.)/116.\n","    x_int = (lab[:,1,:,:]/500.) + y_int\n","    z_int = y_int - (lab[:,2,:,:]/200.)\n","    if(z_int.is_cuda):\n","        z_int = torch.max(torch.Tensor((0,)).cuda(), z_int)\n","    else:\n","        z_int = torch.max(torch.Tensor((0,)), z_int)\n","\n","    out = torch.cat((x_int[:,None,:,:],y_int[:,None,:,:],z_int[:,None,:,:]),dim=1)\n","    mask = (out > .2068966).type(torch.FloatTensor)\n","    if(out.is_cuda):\n","        mask = mask.cuda()\n","\n","    out = (out**3.)*mask + (out - 16./116.)/7.787*(1-mask)\n","\n","    sc = torch.Tensor((0.95047, 1., 1.08883))[None,:,None,None]\n","    sc = sc.to(out.device)\n","\n","    out = out*sc\n","\n","    # if(torch.sum(torch.isnan(out))>0):\n","        # print('lab2xyz')\n","        # embed()\n","\n","    return out\n","\n","def rgb2lab(rgb, opt):\n","    lab = xyz2lab(rgb2xyz(rgb))\n","    # print(lab[0, 0, 0, 0])\n","    l_rs = (lab[:,[0],:,:]-opt.l_cent)/opt.l_norm\n","    # print(l_rs[0, 0, 0, 0])\n","    ab_rs = lab[:,1:,:,:]/opt.ab_norm\n","    out = torch.cat((l_rs,ab_rs),dim=1)\n","    # if(torch.sum(torch.isnan(out))>0):\n","        # print('rgb2lab')\n","        # embed()\n","    return out\n","\n","def lab2rgb(lab_rs, opt):\n","    l = lab_rs[:,[0],:,:]*opt.l_norm + opt.l_cent\n","    ab = lab_rs[:,1:,:,:]*opt.ab_norm\n","    lab = torch.cat((l,ab),dim=1)\n","    out = xyz2rgb(lab2xyz(lab))\n","    # if(torch.sum(torch.isnan(out))>0):\n","        # print('lab2rgb')\n","        # embed()\n","    return out\n","\n","def get_colorization_data(data_raw, opt, ab_thresh=5., p=.125, num_points=None):\n","    data = {}\n","    data_lab = rgb2lab(data_raw[0], opt)\n","    data['A'] = data_lab[:,[0,],:,:]\n","    data['B'] = data_lab[:,1:,:,:]\n","\n","    if(ab_thresh > 0): # mask out grayscale images\n","        thresh = 1.*ab_thresh/opt.ab_norm\n","        mask = torch.sum(torch.abs(torch.max(torch.max(data['B'],dim=3)[0],dim=2)[0]-torch.min(torch.min(data['B'],dim=3)[0],dim=2)[0]),dim=1) >= thresh\n","        data['A'] = data['A'][mask,:,:,:]\n","        data['B'] = data['B'][mask,:,:,:]\n","        # print('Removed %i points'%torch.sum(mask==0).numpy())\n","        if(torch.sum(mask)==0):\n","            return None\n","\n","    return add_color_patches_rand_gt(data, opt, p=p, num_points=num_points)\n","\n","def add_color_patches_rand_gt(data,opt,p=.125,num_points=None,use_avg=True,samp='normal'):\n","# Add random color points sampled from ground truth based on:\n","#   Number of points\n","#   - if num_points is 0, then sample from geometric distribution, drawn from probability p\n","#   - if num_points > 0, then sample that number of points\n","#   Location of points\n","#   - if samp is 'normal', draw from N(0.5, 0.25) of image\n","#   - otherwise, draw from U[0, 1] of image\n","    N,C,H,W = data['B'].shape\n","\n","    data['hint_B'] = torch.zeros_like(data['B'])\n","    data['mask_B'] = torch.zeros_like(data['A'])\n","\n","    for nn in range(N):\n","        pp = 0\n","        cont_cond = True\n","        while(cont_cond):\n","            if(num_points is None): # draw from geometric\n","                # embed()\n","                cont_cond = np.random.rand() < (1-p)\n","            else: # add certain number of points\n","                cont_cond = pp < num_points\n","            if(not cont_cond): # skip out of loop if condition not met\n","                continue\n","            print('add hint !!!!!!!!!')\n","            P = np.random.choice(opt.sample_Ps) # patch size\n","\n","            # sample location\n","            if(samp=='normal'): # geometric distribution\n","                h = int(np.clip(np.random.normal( (H-P+1)/2., (H-P+1)/4.), 0, H-P))\n","                w = int(np.clip(np.random.normal( (W-P+1)/2., (W-P+1)/4.), 0, W-P))\n","            else: # uniform distribution\n","                h = np.random.randint(H-P+1)\n","                w = np.random.randint(W-P+1)\n","\n","            # add color point\n","            if(use_avg):\n","                # embed()\n","                data['hint_B'][nn,:,h:h+P,w:w+P] = torch.mean(torch.mean(data['B'][nn,:,h:h+P,w:w+P],dim=2,keepdim=True),dim=1,keepdim=True).view(1,C,1,1)\n","            else:\n","                data['hint_B'][nn,:,h:h+P,w:w+P] = data['B'][nn,:,h:h+P,w:w+P]\n","\n","            data['mask_B'][nn,:,h:h+P,w:w+P] = 1\n","\n","            # increment counter\n","            pp+=1\n","\n","    data['mask_B']-=opt.mask_cent\n","\n","    return data\n","\n","def add_color_patch(data,mask,opt,P=1,hw=[128,128],ab=[0,0]):\n","    # Add a color patch at (h,w) with color (a,b)\n","    data[:,0,hw[0]:hw[0]+P,hw[1]:hw[1]+P] = 1.*ab[0]/opt.ab_norm\n","    data[:,1,hw[0]:hw[0]+P,hw[1]:hw[1]+P] = 1.*ab[1]/opt.ab_norm\n","    mask[:,:,hw[0]:hw[0]+P,hw[1]:hw[1]+P] = 1-opt.mask_cent\n","\n","    return (data,mask)\n","\n","def crop_mult(data,mult=16,HWmax=[800,1200]):\n","    # crop image to a multiple\n","    H,W = data.shape[2:]\n","    Hnew = int(min(H/mult*mult,HWmax[0]))\n","    Wnew = int(min(W/mult*mult,HWmax[1]))\n","    h = (H-Hnew)/2\n","    w = (W-Wnew)/2\n","\n","    return data[:,:,h:h+Hnew,w:w+Wnew]\n","\n","def encode_ab_ind(data_ab, opt):\n","    # Encode ab value into an index\n","    # INPUTS\n","    #   data_ab   Nx2xHxW \\in [-1,1]\n","    # OUTPUTS\n","    #   data_q    Nx1xHxW \\in [0,Q)\n","\n","    data_ab_rs = torch.round((data_ab*opt.ab_norm + opt.ab_max)/opt.ab_quant) # normalized bin number\n","    data_q = data_ab_rs[:,[0],:,:]*opt.A + data_ab_rs[:,[1],:,:]\n","    return data_q\n","\n","def decode_ind_ab(data_q, opt):\n","    # Decode index into ab value\n","    # INPUTS\n","    #   data_q      Nx1xHxW \\in [0,Q)\n","    # OUTPUTS\n","    #   data_ab     Nx2xHxW \\in [-1,1]\n","\n","    data_a = data_q/opt.A\n","    data_b = data_q - data_a*opt.A\n","    data_ab = torch.cat((data_a,data_b),dim=1)\n","\n","    if(data_q.is_cuda):\n","        type_out = torch.cuda.FloatTensor\n","    else:\n","        type_out = torch.FloatTensor\n","    data_ab = ((data_ab.type(type_out)*opt.ab_quant) - opt.ab_max)/opt.ab_norm\n","\n","    return data_ab\n","\n","def decode_max_ab(data_ab_quant, opt):\n","    # Decode probability distribution by using bin with highest probability\n","    # INPUTS\n","    #   data_ab_quant   NxQxHxW \\in [0,1]\n","    # OUTPUTS\n","    #   data_ab         Nx2xHxW \\in [-1,1]\n","\n","    data_q = torch.argmax(data_ab_quant,dim=1)[:,None,:,:]\n","    return decode_ind_ab(data_q, opt)\n","\n","def decode_mean(data_ab_quant, opt):\n","    # Decode probability distribution by taking mean over all bins\n","    # INPUTS\n","    #   data_ab_quant   NxQxHxW \\in [0,1]\n","    # OUTPUTS\n","    #   data_ab_inf     Nx2xHxW \\in [-1,1]\n","\n","    (N,Q,H,W) = data_ab_quant.shape\n","    a_range = torch.range(-opt.ab_max, opt.ab_max, step=opt.ab_quant).to(data_ab_quant.device)[None,:,None,None]\n","    a_range = a_range.type(data_ab_quant.type())\n","\n","    # reshape to AB space\n","    data_ab_quant = data_ab_quant.view((N,int(opt.A),int(opt.A),H,W))\n","    data_a_total = torch.sum(data_ab_quant,dim=2)\n","    data_b_total = torch.sum(data_ab_quant,dim=1)\n","\n","    # matrix multiply\n","    data_a_inf = torch.sum(data_a_total * a_range,dim=1,keepdim=True)\n","    data_b_inf = torch.sum(data_b_total * a_range,dim=1,keepdim=True)\n","\n","    data_ab_inf = torch.cat((data_a_inf,data_b_inf),dim=1)/opt.ab_norm\n","\n","    return data_ab_inf\n","\n","def calculate_psnr_np(img1, img2):\n","    import numpy as np\n","    SE_map = (1.*img1-img2)**2\n","    cur_MSE = np.mean(SE_map)\n","    return 20*np.log10(255./np.sqrt(cur_MSE))\n","\n","def calculate_psnr_torch(img1, img2):\n","    SE_map = (1.*img1-img2)**2\n","    cur_MSE = torch.mean(SE_map)\n","    return 20*torch.log10(1./torch.sqrt(cur_MSE))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ITqJN0tzOokW"},"source":["image pool"]},{"cell_type":"code","metadata":{"id":"Br9vWpzAOr76"},"source":["import random\n","import torch\n","\n","\n","class ImagePool():\n","    def __init__(self, pool_size):\n","        self.pool_size = pool_size\n","        if self.pool_size > 0:\n","            self.num_imgs = 0\n","            self.images = []\n","\n","    def query(self, images):\n","        if self.pool_size == 0:\n","            return images\n","        return_images = []\n","        for image in images:\n","            image = torch.unsqueeze(image.data, 0)\n","            if self.num_imgs < self.pool_size:\n","                self.num_imgs = self.num_imgs + 1\n","                self.images.append(image)\n","                return_images.append(image)\n","            else:\n","                p = random.uniform(0, 1)\n","                if p > 0.5:\n","                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive\n","                    tmp = self.images[random_id].clone()\n","                    self.images[random_id] = image\n","                    return_images.append(tmp)\n","                else:\n","                    return_images.append(image)\n","        return_images = torch.cat(return_images, 0)\n","        return return_images"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NSW5XQ0KNV_o"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"V0IrsxtYMe77"},"source":["import time\n","from options.train_options import TrainOptions\n","from models import create_model\n","# from util.visualizer import Visualizer\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from tqdm import trange, tqdm\n","\n","from fusion_dataset import *\n","from util import util\n","import os\n","\n","if __name__ == '__main__':\n","    opt = TrainOptions().parse()\n","    if opt.stage == 'full':\n","        dataset = Training_Full_Dataset(opt)\n","    elif opt.stage == 'instance':\n","        dataset = Training_Instance_Dataset(opt)\n","    elif opt.stage == 'fusion':\n","        dataset = Training_Fusion_Dataset(opt)\n","    else:\n","        print('Error! Wrong stage selection!')\n","        exit()\n","    dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=opt.batch_size, shuffle=True, num_workers=8)\n","\n","    dataset_size = len(dataset)\n","    print('#training images = %d' % dataset_size)\n","\n","    model = create_model(opt)\n","    model.setup(opt)\n","\n","    # opt.display_port = 8098\n","    # visualizer = Visualizer(opt)\n","    total_steps = 0\n","\n","    if opt.stage == 'full' or opt.stage == 'instance':\n","        for epoch in trange(opt.epoch_count, opt.niter + opt.niter_decay, desc='epoch', dynamic_ncols=True):\n","            epoch_iter = 0\n","\n","            for data_raw in tqdm(dataset_loader, desc='batch', dynamic_ncols=True, leave=False):\n","                total_steps += opt.batch_size\n","                epoch_iter += opt.batch_size\n","\n","                data_raw['rgb_img'] = [data_raw['rgb_img']]\n","                data_raw['gray_img'] = [data_raw['gray_img']]\n","\n","                input_data = util.get_colorization_data(data_raw['gray_img'], opt, p=1.0, ab_thresh=0)\n","                gt_data = util.get_colorization_data(data_raw['rgb_img'], opt, p=1.0, ab_thresh=10.0)\n","                if gt_data is None:\n","                    continue\n","                if(gt_data['B'].shape[0] < opt.batch_size):\n","                    continue\n","                input_data['B'] = gt_data['B']\n","                input_data['hint_B'] = gt_data['hint_B']\n","                input_data['mask_B'] = gt_data['mask_B']\n","\n","                # visualizer.reset()\n","                model.set_input(input_data)\n","                model.optimize_parameters()\n","\n","                if total_steps % opt.display_freq == 0:\n","                    save_result = total_steps % opt.update_html_freq == 0\n","                    # visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)\n","\n","                if total_steps % opt.print_freq == 0:\n","                    losses = model.get_current_losses()\n","                    # if opt.display_id > 0:\n","                        # visualizer.plot_current_losses(epoch, float(epoch_iter) / dataset_size, opt, losses)\n","\n","            if epoch % opt.save_epoch_freq == 0:\n","                model.save_networks('latest')\n","                model.save_networks(epoch)\n","            model.update_learning_rate()\n","    elif opt.stage == 'fusion':\n","        for epoch in trange(opt.epoch_count, opt.niter + opt.niter_decay, desc='epoch', dynamic_ncols=True):\n","            epoch_iter = 0\n","\n","            for data_raw in tqdm(dataset_loader, desc='batch', dynamic_ncols=True, leave=False):\n","                total_steps += opt.batch_size\n","                epoch_iter += opt.batch_size\n","                box_info = data_raw['box_info'][0]\n","                box_info_2x = data_raw['box_info_2x'][0]\n","                box_info_4x = data_raw['box_info_4x'][0]\n","                box_info_8x = data_raw['box_info_8x'][0]\n","                cropped_input_data = util.get_colorization_data(data_raw['cropped_gray'], opt, p=1.0, ab_thresh=0)\n","                cropped_gt_data = util.get_colorization_data(data_raw['cropped_rgb'], opt, p=1.0, ab_thresh=10.0)\n","                full_input_data = util.get_colorization_data(data_raw['full_gray'], opt, p=1.0, ab_thresh=0)\n","                full_gt_data = util.get_colorization_data(data_raw['full_rgb'], opt, p=1.0, ab_thresh=10.0)\n","                if cropped_gt_data is None or full_gt_data is None:\n","                    continue\n","                cropped_input_data['B'] = cropped_gt_data['B']\n","                full_input_data['B'] = full_gt_data['B']\n","                # visualizer.reset()\n","                model.set_input(cropped_input_data)\n","                model.set_fusion_input(full_input_data, [box_info, box_info_2x, box_info_4x, box_info_8x])\n","                model.optimize_parameters()\n","\n","                if total_steps % opt.display_freq == 0:\n","                    save_result = total_steps % opt.update_html_freq == 0\n","                    # visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)\n","\n","                if total_steps % opt.print_freq == 0:\n","                    losses = model.get_current_losses()\n","                    if opt.display_id > 0:\n","                        # visualizer.plot_current_losses(epoch, float(epoch_iter) / dataset_size, opt, losses)\n","            if epoch % opt.save_epoch_freq == 0:\n","                model.save_fusion_epoch(epoch)\n","            model.update_learning_rate()\n","    else:\n","        print('Error! Wrong stage selection!')\n","        exit()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xsAYJ1-3TqbC"},"source":["# Trainig scripts"]},{"cell_type":"code","metadata":{"id":"Hcp6lwRGH66o"},"source":["DATASET_DIR='train_data/cocostuff8000'\n","STAGE_1_NAME = 'coco_full1'\n","STAGE_2_NAME = 'coco_instance1'\n","STAGE_3_NAME = 'coco_mask1'\n","# Stage 1: Training Full Image Colorization\n","!mkdir ./checkpoints/$STAGE_1_NAME\n","!cp ./checkpoints/siggraph_retrained/latest_net_G.pth ./checkpoints/$STAGE_1_NAME/\n","!python train.py --stage full --name $STAGE_1_NAME --sample_p 1.0 --niter 2 --niter_decay 0 --load_model --lr 0.0005 --model train --fineSize 256 --batch_size 16 --display_ncols 3 --display_freq 1600 --print_freq 1600 --train_img_dir $DATASET_DIR\n","\n","# Stage 2: Training Instance Image Colorization\n","!mkdir ./checkpoints/$STAGE_2_NAME\n","!cp ./checkpoints/$STAGE_1_NAME/latest_net_G.pth ./checkpoints/$STAGE_2_NAME/\n","!python train.py --stage instance --name $STAGE_2_NAME --sample_p 1.0 --niter 5 --niter_decay 0 --load_model --lr 0.0005 --model train --fineSize 256 --batch_size 16 --display_ncols 3 --display_freq 1600 --print_freq 1600 --train_img_dir $DATASET_DIR\n","\n","# Stage 3: Training Fusion Module\n","!mkdir ./checkpoints/$STAGE_3_NAME\n","!cp ./checkpoints/$STAGE_1_NAME/latest_net_G.pth ./checkpoints/$STAGE_3_NAME/latest_net_GF.pth\n","!cp ./checkpoints/$STAGE_2_NAME/latest_net_G.pth ./checkpoints/$STAGE_3_NAME/latest_net_G.pth\n","!cp ./checkpoints/$STAGE_1_NAME/latest_net_G.pth ./checkpoints/$STAGE_3_NAME/latest_net_GComp.pth\n","!python train.py --stage fusion --name $STAGE_3_NAME --sample_p 1.0 --niter 2 --niter_decay 0 --lr 0.00005 --model train --load_model --display_ncols 4 --fineSize 256 --batch_size 1 --display_freq 500 --print_freq 500 --train_img_dir $DATASET_DIR"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"1Ikk4kGRu4F-"}},{"cell_type":"code","metadata":{"id":"iRb2OJBrXF7z"},"source":["DATASET_DIR='train_data/cocostuff8000'\n","STAGE_1_NAME = 'coco_full1'\n","STAGE_2_NAME = 'coco_instance1'\n","STAGE_3_NAME = 'coco_mask2'\n","\n","# Stage 3: Training Fusion Module\n","!mkdir ./checkpoints/$STAGE_3_NAME\n","!cp ./checkpoints/$STAGE_1_NAME/latest_net_G.pth ./checkpoints/$STAGE_3_NAME/latest_net_GF.pth\n","!cp ./checkpoints/$STAGE_2_NAME/latest_net_G.pth ./checkpoints/$STAGE_3_NAME/latest_net_G.pth\n","!cp ./checkpoints/$STAGE_1_NAME/latest_net_G.pth ./checkpoints/$STAGE_3_NAME/latest_net_GComp.pth\n","!python train.py --beta1 0.99 --stage fusion --name $STAGE_3_NAME --sample_p 1.0 --niter 2 --niter_decay 0 --lr 0.00002 --model train --load_model --display_ncols 4 --fineSize 256 --batch_size 1 --display_freq 500 --print_freq 500 --train_img_dir $DATASET_DIR"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PMeFGrAQLi_x"},"source":["DATASET_DIR='train_data/cocostuff8000'\n","STAGE_1_NAME = 'coco_full1'\n","STAGE_2_NAME = 'coco_instance1'\n","STAGE_3_NAME = 'coco_mask3'\n","\n","# Stage 3: Training Fusion Module\n","!mkdir ./checkpoints/$STAGE_3_NAME\n","!cp ./checkpoints/$STAGE_1_NAME/latest_net_G.pth ./checkpoints/$STAGE_3_NAME/latest_net_GF.pth\n","!cp ./checkpoints/$STAGE_2_NAME/latest_net_G.pth ./checkpoints/$STAGE_3_NAME/latest_net_G.pth\n","!cp ./checkpoints/$STAGE_1_NAME/latest_net_G.pth ./checkpoints/$STAGE_3_NAME/latest_net_GComp.pth\n","!python train.py --init_type kaiming --beta1 0.99 --stage fusion --name $STAGE_3_NAME --sample_p 1.0 --niter 2 --niter_decay 0 --lr 0.00002 --model train --load_model --display_ncols 4 --fineSize 256 --batch_size 1 --display_freq 500 --print_freq 500 --train_img_dir $DATASET_DIR"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B-8YpOikSj6f","executionInfo":{"status":"ok","timestamp":1637243343711,"user_tz":-60,"elapsed":243,"user":{"displayName":"Ding Yifei","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14157177939779618244"}},"outputId":"fae6d501-0cd6-424d-bf25-a000ace3d5a1"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["drive  sample_data\n"]}]},{"cell_type":"markdown","metadata":{"id":"WnZXbI5nGAdO"},"source":["# Testing scripts"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5V0ApsCOGDJu","executionInfo":{"status":"ok","timestamp":1637442837726,"user_tz":-60,"elapsed":1905779,"user":{"displayName":"Ding Yifei","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14157177939779618244"}},"outputId":"3892d924-bf05-48b1-81df-1ac5dca79f1a"},"source":["INPUT_DIR='test/cocostuff2000'\n","OUTPUT_DIR='test/cocostuff2000output'\n","\n","!python inference_bbox.py --test_img_dir $INPUT_DIR\n","!python test_fusion.py --name test_fusion1 --sample_p 1.0 --model fusion --fineSize 256 --test_img_dir $INPUT_DIR --results_img_dir $OUTPUT_DIR"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["model_final_2d9806.pkl: 431MB [00:04, 90.1MB/s]               \n","Create path: test/cocostuff2000_bbox\n","100% 2000/2000 [17:15<00:00,  1.93it/s]\n","#Testing images = 2000\n","initialize network with normal\n","initialize network with normal\n","initialize network with normal\n","model [FusionModel] was created\n","load Fusion model from checkpoints/coco_finetuned_mask_256_ffs/latest_net_GF.pth\n","  0% 0/2000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode))\n","100% 2000/2000 [13:46<00:00,  2.42it/s]\n","10 images without bounding boxes\n"]}]}]}